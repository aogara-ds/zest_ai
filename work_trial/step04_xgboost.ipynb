{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac407935",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d58c7e",
   "metadata": {},
   "source": [
    "This notebook implements a gradient-boosted decision tree ensemble via XGBoost. The model uses 63 input features to predict the single binary outcome of whether or not a particular loan is bad. 1771 models are built with unique hyperparameters to find the model structure that maximizes performance on key target metrics. The highest performing model correctly identifies whether a loan will be good or bad in 6% of validation set cases. \n",
    "\n",
    "Data is split into three sets: train, validation, and test. Individual models are fitted on the training data. Performance metrics are reported on validation data, which is not used in training. Once a model has been chosen, test data will provide an estimate of its performance in future use. Cross-validation is used to generate five unique splits of training and validation data for use in fitting, providing performance estimates less biased by the individual dataset. \n",
    "\n",
    "Six hyperparameters are tuned in order to optimize performance of the model (tested values in parentheses):\n",
    "* Maximum Tree Depth: How many levels of splits can an individual tree contain? (2, 4, 8, 16, 32)\n",
    "* Minimum Child Weight: What is the minimum sample size of a new node/leaf? (1, 4, 8, 12, 16, 20, 24, 48)\n",
    "* Subsample Fraction: What fraction of the training data is each tree trained on? (0.70, 1.00)\n",
    "* Lambda: Weight multiplier on the L2 regularization parameter. (0, 0.5, 1)\n",
    "* Gamma: Prune nodes below the given threshold of performance gain. (0, 0.5, 1, 5, 10, 50, 100)\n",
    "* Eta: Reduce the weight of each successive tree by the given multiplier. (0.001, 0.01, 0.05, 0.1, 0.25, 0.5)\n",
    "\n",
    "Six model performance metrics are tracked for analysis in the next notebook:\n",
    "* Brier Score\n",
    "* AUC-ROC Curve\n",
    "    * TODO: Plot this\n",
    "* Precision\n",
    "* Recall\n",
    "* Accuracy\n",
    "* Balanced Accuracy\n",
    "\n",
    "Future work on this question would include the specification of a custom scoring mechanism for fair lending metrics. By deriving each customer's demographic data from their application information, we could measure the default probabilities assigned to customers of different protected groups. We could compare their overall probabilities of default, as well as probabilities of false positive and false negative, in order to estimate the fair lending impact of our model.\n",
    "\n",
    "\n",
    "Paper on XGBoost for Default Prediction: https://www.terry.uga.edu/sites/default/files/inline-files/Albanesi_Domossy_2021.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c985f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import xgboost\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c302b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and data dictionary\n",
    "data = pd.read_pickle('output_data/02_data.pkl')\n",
    "\n",
    "data_dict = pd.read_pickle('output_data/02_data_dict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d7457",
   "metadata": {},
   "source": [
    "## Train, Test, and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533315a",
   "metadata": {},
   "source": [
    "TODO: Fix description. I'm going to start with a train-validation-test split of 60/20/20. This is weighted more towards test data and less towards training data than others splits that are commonly used. This decision is motivated by concern of overfitting with a powerful model predicting such a small sample size. With a sizeable test set, we should have a clear understanding of how well the model performs on the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b759898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate feature set\n",
    "\n",
    "feature_set = {\n",
    "    'all': data_dict.loc[data_dict['potential_feature']==True, \n",
    "                               'variable'].values,\n",
    "    'no_credit': data_dict.loc[data_dict['eda_category'].isin(('personal_finance', 'other_info')),\n",
    "                               'variable'].values,\n",
    "    'significant': data_dict.loc[data_dict['pearson_p'] < 0.05, \n",
    "                               'variable'].values\n",
    "    'significant_no_credit': data_dict.loc[(data_dict['pearson_p'] < 0.05) &\n",
    "        (data_dict['eda_category'].isin(('personal_finance', 'other_info'))),\n",
    "                                                          'variable'].values,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a05d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chronological method achieves Train-Val-Test split of approximately 85-15 proportions.\n",
      "Validation sets consisting of 20% of training data will be dynamically generated later.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train    555\n",
       "test      95\n",
       "Name: model_set, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training, validation, and testing data\n",
    "# Split chronologically to resolve the look-ahead problem\n",
    "\n",
    "# Generate a new column to track the observation's model dataset category\n",
    "data['model_set'] = 'train'\n",
    "\n",
    "# Test Data comes from March 2011\n",
    "data.loc[data['application_month_of_cycle'] == 6, 'model_set'] = 'test'\n",
    "\n",
    "# Display results\n",
    "print(\"Chronological method achieves Train-Val-Test split of approximately 85-15 proportions.\")\n",
    "print(\"Validation sets consisting of 20% of training data will be dynamically generated later.\")\n",
    "data['model_set'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea4a28",
   "metadata": {},
   "source": [
    "TODO: PowerPoint chart to explain where we divided the train, test, and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74b2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into x and y data\n",
    "# Use the list of potential features from the data dictionary\n",
    "X_train = data.loc[data['model_set'] == 'train']\n",
    "X_test = data.loc[data['model_set'] == 'test']\n",
    "\n",
    "# Designate Y data for train, validation, and test\n",
    "y_train = data.loc[data['model_set'] == 'train', 'bad']\n",
    "y_test = data.loc[data['model_set'] == 'test', 'bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e830afd",
   "metadata": {},
   "source": [
    "## Sample Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be7b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure constant model settings\n",
    "settings = {\n",
    "    'booster': ['gbtree'], \n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'], # TODO: Suppress the warning down there. \n",
    "    'nthread': [multiprocessing.cpu_count()]\n",
    "}\n",
    "\n",
    "# Configure hyperparameters\n",
    "# Currently building 1728 models\n",
    "hypers = {\n",
    "    'eta': [0.01, .10, .25, .50], # learning rate, reduces size of error adjustment\n",
    "    'gamma': [0, 0.5, 5], # Raise to reduce overfitting (prevents low-magnitude leaves)    \n",
    "    'min_child_weight': [1, 4, 16], # Raise to eliminate leaves with low sample weight\n",
    "    'subsample': [.7, 1], # Each boosting round trains on X% of the training data\n",
    "    'max_depth': [2, 4, 8], # Maximum depth of a single tree\n",
    "    'reg_lambda': [0, 1], # L2 regularization\n",
    "}\n",
    "\n",
    "single_hyper = {k: v[0] for k, v in hypers.items()}\n",
    "\n",
    "# Monitor hypers chosen by XGB\n",
    "monitor_hypers = {\n",
    "    'num_pbuffer': None,\n",
    "    'num_feature': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b6b8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dict = settings.copy()\n",
    "new_simple_dict = {\n",
    "    'eta': [.10], # learning rate, reduces size of error adjustment\n",
    "    'gamma': [0], # Raise to reduce overfitting (prevents low-magnitude leaves)    \n",
    "    'min_child_weight': [1, 4, 16], # Raise to eliminate leaves with low sample weight\n",
    "    'subsample': [.7], # Each boosting round trains on X% of the training data\n",
    "    'max_depth': [2, 4, 8], # Maximum depth of a single tree\n",
    "    'reg_lambda': [0, 1], # L2 regularization\n",
    "}\n",
    "simple_dict.update(new_simple_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8364771d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5368421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "tree = XGBClassifier()\n",
    "params = settings.update(hypers)\n",
    "num_round = [2, 4, 8, 16]\n",
    "\n",
    "tree = tree.fit(X = X_train[feature_set['significant_no_credit']], \n",
    "                y = y_train,\n",
    "                eval_metric='rmse')\n",
    "\n",
    "y_pred = tree.predict(X_test[feature_set['significant_no_credit']])\n",
    "\n",
    "print(f\"Accuracy: {(y_test == y_pred).sum() / (y_test == y_pred).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5ea53",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4687f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccb71a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure constant model settings\n",
    "params = {\n",
    "    'booster': ['gbtree'], \n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'], # TODO: Suppress the warning down there. \n",
    "    'nthread': [multiprocessing.cpu_count()],\n",
    "    'eta': [0.01, .10, .25, .50], # learning rate, reduces size of error adjustment\n",
    "    'gamma': [0, 0.5, 5], # Raise to reduce overfitting (prevents low-magnitude leaves)    \n",
    "    'min_child_weight': [1, 4, 16], # Raise to eliminate leaves with low sample weight\n",
    "    'subsample': [.7, 1], # Each boosting round trains on X% of the training data\n",
    "    'max_depth': [2, 4, 8], # Maximum depth of a single tree\n",
    "    'reg_lambda': [0, 1], # L2 regularization\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8772a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=params,\n",
    "    scoring = ['neg_brier_score', 'accuracy', 'balanced_accuracy', 'precision', 'recall', 'roc_auc'],\n",
    "    n_jobs = multiprocessing.cpu_count(),\n",
    "    cv = 5,\n",
    "    verbose=10,\n",
    "    refit = 'neg_brier_score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06b87aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# For each potential set of features\n",
    "for name, features in feature_set.items():\n",
    "    \n",
    "    # Fit each classifier in the grid search\n",
    "    grid_search.fit(X_train[features], y_train)\n",
    "    \n",
    "    # Save performance with dynamic naming\n",
    "    performance = pd.DataFrame(grid_search.cv_results_)\n",
    "    performance.to_pickle(f'output_data/04_{name}_performance_0001.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88006",
   "metadata": {},
   "source": [
    "## Second Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b275d7d",
   "metadata": {},
   "source": [
    "After a round of performance evaluation, I've picked some hyperparameters and come back to test more. Thanks to the scoring of our earlier models, we know our tuning is improving model performance on the validation set. \n",
    "\n",
    "Here are the hyperparameters we'd like to continue improving upon:\n",
    "\n",
    "* Maximum Depth of a Tree\n",
    "* Minimum Weight of a Leaf\n",
    "* Gamma Parameter\n",
    "* Eta Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b1107fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_0002 = {\n",
    "    'booster': ['gbtree'], \n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'], # TODO: Suppress the warning down there. \n",
    "    'nthread': [multiprocessing.cpu_count()],\n",
    "    'eta': [.05, .01, .001, .0001], \n",
    "    'gamma': [1, 10, 50, 100], # Raise to reduce overfitting (prevents low-magnitude leaves)    \n",
    "    'min_child_weight': [8, 24, 48], # Raise to eliminate leaves with low sample weight\n",
    "    'max_depth': [4, 8, 16, 32], # Maximum depth of a single tree\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fff566ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=params_0002,\n",
    "    scoring = ['neg_brier_score', 'accuracy', 'balanced_accuracy', 'precision', 'recall', 'roc_auc'],\n",
    "    n_jobs = multiprocessing.cpu_count(),\n",
    "    cv = 5,\n",
    "    verbose=10,\n",
    "    refit = 'neg_brier_score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c3cc232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# For each potential set of features\n",
    "for name, features in feature_set.items():\n",
    "    \n",
    "    # Fit each classifier in the grid search\n",
    "    grid_search.fit(X_train[features], y_train)\n",
    "    \n",
    "    # Save performance with dynamic naming\n",
    "    performance = pd.DataFrame(grid_search.cv_results_)\n",
    "    performance.to_pickle(f'output_data/04_{name}_performance_0002.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c1502",
   "metadata": {},
   "source": [
    "## Third Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a2a3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure constant model settings\n",
    "params_0003 = {\n",
    "    'booster': ['gbtree'], \n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'], # TODO: Suppress the warning down there. \n",
    "    'nthread': [multiprocessing.cpu_count()],\n",
    "    'eta': [0.08, .12, .0005, .4], # learning rate, reduces size of error adjustment\n",
    "    'gamma': [0.25, .75, 2, 8], # Raise to reduce overfitting (prevents low-magnitude leaves)    \n",
    "    'min_child_weight': [8, 12, 20], # Raise to eliminate leaves with low sample weight\n",
    "    'subsample': [.7, 1], # Each boosting round trains on X% of the training data\n",
    "    'max_depth': [2, 4, 8], # Maximum depth of a single tree\n",
    "    'reg_lambda': [0, 0.5, 1, 1.5], # L2 regularization\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6be53642",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=params_0003,\n",
    "    scoring = ['neg_brier_score', 'accuracy', 'balanced_accuracy', 'precision', 'recall', 'roc_auc'],\n",
    "    n_jobs = multiprocessing.cpu_count(),\n",
    "    cv = 5,\n",
    "    verbose=10,\n",
    "    refit = 'neg_brier_score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c52a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1152 candidates, totalling 5760 fits\n"
     ]
    }
   ],
   "source": [
    "# For each potential set of features\n",
    "for name, features in feature_set.items():\n",
    "    \n",
    "    # Fit each classifier in the grid search\n",
    "    grid_search.fit(X_train[features], y_train)\n",
    "    \n",
    "    # Save performance with dynamic naming\n",
    "    performance = pd.DataFrame(grid_search.cv_results_)\n",
    "    performance.to_pickle(f'output_data/04_{name}_performance_0003.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439941d6",
   "metadata": {},
   "source": [
    "## Show ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe0026d",
   "metadata": {},
   "source": [
    "Using a previously estimated model saved as a pickle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f7126c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_0003 = pickle.load(open(\"output_data/04_best_estimator_0003.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02300a99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 63, got 11",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ffb10fc4a82f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mRocCurveDisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_estimator_0003\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'significant_no_credit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_plot\\roc_curve.py\u001b[0m in \u001b[0;36mfrom_estimator\u001b[1;34m(cls, estimator, X, y, sample_weight, drop_intermediate, response_method, pos_label, name, ax, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         y_pred, pos_label = _get_response(\n\u001b[0m\u001b[0;32m    232\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_plot\\base.py\u001b[0m in \u001b[0;36m_get_response\u001b[1;34m(X, estimator, response_method, pos_label)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mprediction_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_classifier_response_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1346\u001b[0m         \u001b[1;31m# binary:logistic: Expand the prob vector into 2-class matrix after predict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m         \u001b[1;31m# binary:logitraw: Unsupported by predict_proba()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m         class_probs = super().predict(\n\u001b[0m\u001b[0;32m   1349\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m             \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"multi:softmax\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m                 predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[0;32m    882\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m                     \u001b[0miteration_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2016\u001b[0m                 )\n\u001b[0;32m   2017\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2018\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   2019\u001b[0m                     \u001b[1;34mf\"Feature shape mismatch, expected: {self.num_features()}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m                     \u001b[1;34mf\"got {data.shape[1]}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Feature shape mismatch, expected: 63, got 11"
     ]
    }
   ],
   "source": [
    "RocCurveDisplay.from_estimator(best_estimator_0003, X_test[feature_set['significant_no_credit']], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279a1ce",
   "metadata": {},
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "X_train.to_pickle('output_data/04_X_train.pkl')\n",
    "X_test.to_pickle('output_data/04_X_test.pkl')\n",
    "\n",
    "y_train.to_pickle('output_data/04_y_train.pkl')\n",
    "y_test.to_pickle('output_data/04_y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best estimator\n",
    "best_estimator = grid_search.best_estimator_\n",
    "pickle.dump(best_estimator, open(\"output_data/04_best_estimator_0003.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a65190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d2d854f",
   "metadata": {},
   "source": [
    "## Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84857faf",
   "metadata": {},
   "source": [
    "Gradient boosting improvements. XGBoost allows you to place weights on the importance of each data point. Bad loans are more important than good, and some data points are harder to classify than others. Give those data points more weight. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
